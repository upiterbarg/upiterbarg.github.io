---
layout: post
title:  diff History for Long-Context Language Agents
date:   2023-05-30 00:00:00 +00:00
image: /diff.png
categories: research
author: "Ulyana Piterbarg, Lerrel Pinto, Rob Fergus"
authors: "<b>Ulyana Piterbarg</b>, Lerrel Pinto, Rob Fergus"
arxiv: https://arxiv.org/abs/2312.07540
---
Language Models (LMs) offer an exciting solution for general-purpose embodied control. However, a key technical issue arises when using an LM-based controller: environment observations must be converted to text, which coupled with history, leads to prohibitively large textual prompts. As a result, prior work in LM agents is limited to restricted domains with either small observation size or minimal needs for interaction history. In this paper, we introduce a simple and highly effective solution to these issues. We exploit the fact that consecutive text observations have high similarity and propose to compress them via the Unix diff command. We demonstrate our approach in NetHack, a complex rogue-like video game, that requires long-horizon reasoning for decision-making and is far from solved, particularly for neural agents. Diff history offers an average of 4x increase in the length of the text-based interaction history available to the LM. This observational compression along with the benefits of abstraction yields a 7x improvement in game score on held-out environment instances over state-of-the-art baselines. It also outperforms prior agents that use visual observations by over 40%.