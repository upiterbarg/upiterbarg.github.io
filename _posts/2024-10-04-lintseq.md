---
layout: post
title:  Training Language Models on Synthetic Edit Sequences Improves Code Synthesis
date:   2024-10-04 00:00:00 +00:00
image: /lintseq.png
categories: research
author: "Ulyana Piterbarg, Lerrel Pinto, Rob Fergus"
authors: "<b>Ulyana Piterbarg</b>, Lerrel Pinto, Rob Fergus"
venue: Preprint
code: https://github.com/upiterbarg/lintseq
projectpage: https://lintseq.github.io/
arxiv: https://arxiv.org/abs/2410.02749
---
There are infinitely many ways to write a program. Training autoregressive LMs to natively synthesize programs with diffs via SFT improves the trade-off between generation quality and inference-time compute. We show that repeatedly sampling solutions to coding problems from small language models tuned on synthetic program-diff sequences yields benchmark coverage that is competitive with GPT-4 and GPT-4-Omni, with similar total cost to generating a single completion from the best open-source LLMs like Llama 3.1 405B.
