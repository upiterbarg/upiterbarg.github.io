---
layout: post
title:  Training Language Models on Synthetic Edit Sequences Improves Code Synthesis
date:   2025-01-22 00:00:00 +00:00
image: /lintseq_2.png
categories: research
author: "Ulyana Piterbarg, Lerrel Pinto, Rob Fergus"
authors: "<b>Ulyana Piterbarg</b>, Lerrel Pinto, Rob Fergus"
venue: "Thirteenth International Conference on Learning Representations (ICLR) "
code: https://github.com/upiterbarg/lintseq
projectpage: https://lintseq.github.io/
arxiv: https://arxiv.org/abs/2410.02749
---
The key to solving a hard problem often lies in knowing how to decompose it into sub-problems. We show that training autoregressive LMs to iteratively synthesize code edit-by-edit improves the slope of test-time scaling laws on benchmarks like HumanEval, MBPP, and CodeContests. Our approach introduces an algorithm for generating synthetic code edit data at scale: LintSeq. This algorithm uses a linter to sample across interdependent lines of existing source code. Edits sampled with LintSeq reflect the semantics & syntax of their programming language. Our method improves performance of LMs ranging in scale from 150M to 14B parameters.