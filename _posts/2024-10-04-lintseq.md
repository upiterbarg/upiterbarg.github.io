---
layout: post
title:  Training Language Models on Synthetic Edit Sequences Improves Code Synthesis
date:   2025-01-22 00:00:00 +00:00
image: /lintseq_2.png
categories: research
author: "Ulyana Piterbarg, Lerrel Pinto, Rob Fergus"
authors: "<b>Ulyana Piterbarg</b>, Lerrel Pinto, Rob Fergus"
venue: "ICLR, <a href=https://nenlp.github.io/spr2025>NENLP</a> (<b>Outstanding Paper Award</b>)"
code: https://github.com/upiterbarg/lintseq
projectpage: https://lintseq.github.io/
arxiv: https://arxiv.org/abs/2410.02749
---
The key to solving a hard problem often lies in knowing how to decompose it into sub-problems. We show that **training autoregressive LMs to synthesize code edit-by-edit improves the slope of test-time scaling laws** on benchmarks like HumanEval, MBPP, and CodeContests. Our approach introduces an algorithm for generating synthetic code edit data at scale: LintSeq. Edits sampled with this algorithm reflect the semantics & syntax of their programming language. 