---
layout: post
title:  Training Language Models on Synthetic Edit Sequences Improves Code Synthesis
date:   2024-10-04 00:00:00 +00:00
image: /lintseq.png
categories: research
author: "Ulyana Piterbarg, Lerrel Pinto, Rob Fergus"
authors: "<b>Ulyana Piterbarg</b>, Lerrel Pinto, Rob Fergus"
venue: Preprint
arxiv: https://arxiv.org/abs/2410.02749
---
LLMs are trained to autoregressively synthesize entire programs from scratch. This makes repeatedly editing a program with an LLM extremely expensive â€“ current state-of-the-art, [LLM-powered code editing tools like Cursor repeatedly prompt models to rewrite entire programs during every edit generation call](https://web.archive.org/web/20240823050616/https://www.cursor.com/blog/instant-apply). **We claim that this is the result of a data problem**. To solve it, we introduce a synthetic data generation algorithm that can be used to re-factor arbitrary code into sequences of error-free code edits. This algorithm is loosely inspired by diffusion modeling and relies on a linter (a standard code analysis tool that statically "verifies" programs) during edit sampling. We show that across parameter scales, finetuning language models on synthetic edit sequences improves the quality and diversity of zero-shot code generations on the code synthesis benchmarks HumanEval and MBPP. **Repeatedly sampling from edit sequence LMs yields coding solutions that are competitive with GPT-4 and GPT-4-Omni**, and have total inference costs that are similar to sampling once from the best open-source LLMs (Llama 3.1 405B, Nemotron 4 340B).
