---
layout: post
title:  Training Language Models on Synthetic Edit Sequences Improves Code Synthesis
date:   2025-01-22 00:00:00 +00:00
image: /lintseq.png
categories: research
author: "Ulyana Piterbarg, Lerrel Pinto, Rob Fergus"
authors: "<b>Ulyana Piterbarg</b>, Lerrel Pinto, Rob Fergus"
venue: "Thirteenth International Conference on Learning Representations (ICLR) "
code: https://github.com/upiterbarg/lintseq
projectpage: https://lintseq.github.io/
arxiv: https://arxiv.org/abs/2410.02749
---
There are infinitely many ways to write a program. Training autoregressive LMs to natively synthesize programs with diffs improves the trade-off between generation quality and inference-time compute. We show that repeatedly sampling solutions to coding problems from small language models SFT-ed on synthetic program diff sequences yields benchmark coverage that is competitive with GPT-4 and GPT-4-Omni, with similar total cost to generating a single completion from the best open-source LLMs like Llama 3.1 405B.
